{
  "hash": "58d050e6df2657c5f322c9af442c5497",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Thinking Like a Developer\"\nsubtitle: \"Performance, Efficiency, and Multi-Language Integration\"\nformat: \n  revealjs:\n    footer: \"[USCOTS Intermediate R Workshop](https://atheobold.github.io/uscots-intermediate-r/)\"\n    theme: [simple, styles.scss]\n    embed-resources: true\n    scrollable: true\n    auto-stretch: true\neditor: source\n---\n\n\n\n\n\n## Why Developer Thinking?\n\n. . .\n\n::: {.incremental}\n- **Performance awareness**: Speed testing and optimization\n- **Code efficiency**: Computational AND cognitive considerations\n- **Multi-language fluency**: R + Python integration\n- **Professional standards**: Maintainable, readable code\n:::\n\n. . .\n\n::: {style=\"font-size: 1.2em; color: #0F4C81;\"}\nToday: Teaching students to think like professional developers\n:::\n\n## Learning Objectives\n\nBy the end of this session, you will:\n\n::: {.incremental}\n1. Understand speed testing principles (good, bad, ugly approaches)\n2. Know how to balance computational and cognitive efficiency\n3. Be familiar with R-Python integration using reticulate and arrow\n:::\n\n. . .\n\n::: {style=\"font-size: 1.1em; color: #2E7D32;\"}\n**Focus: Resources to teach professional development practices**\n:::\n\n\n# Speed Testing {background-color=\"#0F4C81\"}\n\n## The Good, Bad, and Ugly of Performance\n\n**The Good**: Systematic, meaningful optimization\n\n**The Bad**: Premature optimization without evidence\n\n**The Ugly**: Micro-optimizing at the expense of readability\n\n. . .\n\n::: {style=\"font-size: 1.1em; color: #2E7D32;\"}\n**Teaching principle: Measure first, optimize second**\n:::\n\n## Essential Performance Testing Tools\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install performance tools\n# pak::pak(c(\"microbenchmark\", \"profvis\", \"bench\", \"palmerpenguins\"))\n\n# Core performance packages\nlibrary(microbenchmark)  # Micro-benchmarking\nlibrary(profvis)         # Profiling and visualization\nlibrary(bench)           # Modern benchmarking\nlibrary(palmerpenguins)  # Example data\n```\n:::\n\n\n\n\n## The Good: Systematic Benchmarking\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\nlibrary(dplyr)\n\ndt = as.data.table(penguins)\n\n# Compare different approaches systematically\nmicrobenchmark(\n  base_approach = aggregate(body_mass_g ~ species, penguins, mean),\n  dplyr_approach = penguins %>% \n    group_by(species) %>% \n    summarize(mean_mass = mean(body_mass_g, na.rm = TRUE)),\n  data.table_approach = dt[, .(mean_mass = mean(body_mass_g, na.rm = TRUE)), by = species],\n  times = 100\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnit: microseconds\n                expr     min       lq     mean   median       uq      max neval\n       base_approach 268.668 301.7715 341.7602 316.0005 327.7505 1848.792   100\n      dplyr_approach 835.625 872.1045 975.6732 888.4380 920.2300 6758.750   100\n data.table_approach 199.043 220.4585 296.5323 232.3755 247.9175 3203.959   100\n```\n\n\n:::\n:::\n\n\n\n\n## The Bad: Premature Optimization\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# DON'T DO THIS: Optimizing before understanding the problem\n# Spending hours optimizing this:\nfast_but_unreadable <- function(x) {\n  .Call(\"C_fast_mean\", x, PACKAGE = \"mypackage\")\n}\n\n# When this is fast enough and much clearer:\nreadable_solution <- function(x) {\n  mean(x, na.rm = TRUE)\n}\n```\n:::\n\n\n\n\n::: {style=\"font-size: 0.9em; color: #D32F2F;\"}\n**Anti-pattern**: Optimizing code that runs once per analysis and takes 0.001 seconds\n:::\n\n## The Ugly: Sacrificing Readability\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# UGLY: Unreadable \"optimized\" code\nugly_fast <- penguins[!is.na(body_mass_g), \n                     .(m=.Internal(mean(body_mass_g))), \n                     keyby=.(s=species)]\n\n# GOOD: Clear, maintainable code that's fast enough\nclear_code <- penguins %>%\n  filter(!is.na(body_mass_g)) %>%\n  group_by(species) %>%\n  summarize(mean_mass = mean(body_mass_g))\n```\n:::\n\n\n\n\n::: {style=\"font-size: 0.9em; color: #666;\"}\n**Teaching Lesson**: Readability matters more than micro-optimizations\n:::\n\n## Profiling with `profvis`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(profvis)\n\n# set up fake large data\npen2 = replicate(10000, penguins, simplify = FALSE) |> \n  bind_rows() |> \n  mutate(species = paste(species, 1:1000))\n\n# Profile a more complex operation\nprofvis({\n  # Simulate some data processing\n  results <- pen2 |> \n    filter(!is.na(bill_length_mm)) |> \n    group_by(species, island) |> \n    summarize(\n      mean_bill = mean(bill_length_mm),\n      sd_bill = sd(bill_length_mm),\n      .groups = \"drop\"\n    ) |> \n    arrange(desc(mean_bill))\n})\n```\n:::\n\n\n\n\n[profviz docs](https://profvis.r-lib.org/articles/profvis.html)\n\n::: {style=\"font-size: 0.9em; color: #666;\"}\n**Student Activity**: Profile their own code to identify real bottlenecks\n:::\n\n\n## Sample Assignment Prompts\n\n::: {style=\"font-size: 1.2em; color: #666;\"}\n\n* \"Optimize this slow function, but explain why the optimization is worth the complexity\"\n* \"Find the performance bottleneck in this analysis pipeline\"\n* \"Compare three approaches and recommend one for a team project\"\n\n:::\n\n## Performance Testing Teaching Resources\n\n::: {.small}\n**Comprehensive Learning Materials:**\n\n- [Advanced R - Measuring Performance](https://adv-r.hadley.nz/perf-measure.html) - Hadley Wickham's comprehensive guide\n- [Mastering Software Development in R](https://bookdown.org/rdpeng/RProgDA/profiling-and-benchmarking.html) - Academic perspective\n- [USC Biostats R Handbook](https://uscbiostats.github.io/handbook/profile-benchmark.html) - Practical examples\n- [Advanced R Solutions](https://advanced-r-solutions.rbind.io/measuring-performance.html) - Problem-solving approach\n\n**Package Documentation:**\n\n- [microbenchmark Documentation](https://cran.r-project.org/web/packages/microbenchmark/index.html) - Accurate timing functions\n- [Appsilon Microbenchmark Guide](https://www.appsilon.com/post/r-microbenchmark) - Real-world examples\n- [RPubs Efficient R Code](https://rpubs.com/AmarKap/WritingEfficientCodeInR) - Student-friendly tutorial\n\n**Teaching Best Practices:**\n\n- Focus on median times, not minimum\n- Use realistic data sizes for benchmarks\n- Always profile before optimizing\n- Set performance targets before starting\n- *Remember, benchmarking is always built on assumptions!*\n:::\n\n# Code Efficiency {background-color=\"#0F4C81\"}\n\n## Two Types of Efficiency\n\n::: {.midi}\n**Computational Efficiency:**\n\n- How fast does the code run?\n- How much memory does it use?\n- Does it scale well with larger data?\n\n**Cognitive Efficiency:**\n\n- How easy is it to understand?\n- How quickly can someone modify it?\n- How fast can you type it?\n- How likely are bugs to be introduced?\n:::\n\n. . .\n\n::: {style=\"font-size: 1.1em; color: #2E7D32;\"}\n**Key insight: Cognitive efficiency often matters more than computational efficiency**\n:::\n\n## The Readability-Performance Trade-off\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# High readability, adequate performance\nreadable_analysis <- penguins %>%\n  filter(!is.na(bill_length_mm), !is.na(body_mass_g)) %>%\n  mutate(bill_to_mass_ratio = bill_length_mm / body_mass_g) %>%\n  group_by(species) %>%\n  summarize(\n    mean_ratio = mean(bill_to_mass_ratio),\n    median_ratio = median(bill_to_mass_ratio),\n    n_observations = n()\n  )\n\n# Versus optimized but less readable version...\nidx <- !is.na(penguins$bill_length_mm) & !is.na(penguins$body_mass_g)\ns <- penguins$species[idx]\nr <- penguins$bill_length_mm[idx] / penguins$body_mass_g[idx]\nsl <- split(r, s)\n\nvery_ugly_analysis <- data.frame(\n  species = names(sl),\n  mean_ratio = vapply(sl, mean, numeric(1)),\n  median_ratio = vapply(sl, median, numeric(1)),\n  n_observations = vapply(sl, length, integer(1))\n)\n```\n:::\n\n\n\n\n::: {style=\"font-size: 0.9em; color: #666;\"}\n**Teaching Strategy**: Start with readable code, optimize only when necessary\n:::\n\n## Cognitive Load Principles\n\n::: {.midi}\n**Reduce Mental Overhead:**\n\n1. **Meaningful names**: `calculate_species_averages()` not `calc_sp_avg()`\n2. **Consistent style**: Pick one approach and stick to it\n3. **Appropriate abstraction**: Functions for repeated logic\n4. **Clear structure**: Logical flow from input to output\n\n**Research-backed**: Poor code readability increases cognitive load and reduces maintenance efficiency by up to 58%\n:::\n\n\n## Good Cognitive Efficiency Example\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# GOOD: Clear intent, logical flow\nanalyze_penguin_measures <- function(data, measurement_var) {\n  data %>%\n    filter(!is.na({{measurement_var}})) %>%\n    group_by(species, island) %>%\n    summarize(\n      mean_value = mean({{measurement_var}}, na.rm = TRUE),\n      std_dev = sd({{measurement_var}}, na.rm = TRUE),\n      sample_size = n(),\n      .groups = \"drop\"\n    ) %>%\n    arrange(species, island)\n}\n\n# Usage is self-documenting\nbill_analysis <- analyze_penguin_measures(penguins, bill_length_mm)\nmass_analysis <- analyze_penguin_measures(penguins, body_mass_g)\n```\n:::\n\n\n\n\n## Bad Cognitive Efficiency Example\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# BAD: Unclear purpose, complex logic\nf <- function(d, v) {\n  d[!is.na(d[[deparse(substitute(v))]]), ] %>%\n    split(paste(d$species, d$island)) %>%\n    map_dfr(~ data.frame(\n      grp = .x$species[1], \n      isl = .x$island[1],\n      m = mean(.x[[deparse(substitute(v))]]),\n      s = sd(.x[[deparse(substitute(v))]]),\n      n = nrow(.x)\n    ))\n}\n\n# What does this do? How do I use it?\nresult <- f(penguins, bill_length_mm)  # Unclear!\n```\n:::\n\n\n\n\n## Teaching Computational Efficiency\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show students the progression:\n\n# 1. Inefficient but clear\nslow_approach <- function(data) {\n  results <- data.frame()\n  for(species in unique(data$species)) {\n    subset_data <- data[data$species == species, ]\n    mean_mass <- mean(subset_data$body_mass_g, na.rm = TRUE)\n    results <- rbind(results, data.frame(species = species, mean_mass = mean_mass))\n  }\n  return(results)\n}\n\n# 2. Efficient and still clear\nfast_approach <- function(data) {\n  data %>%\n    group_by(species) %>%\n    summarize(mean_mass = mean(body_mass_g, na.rm = TRUE))\n}\n```\n:::\n\n\n\n\n::: {style=\"font-size: 0.9em; color: #666;\"}\n**Pedagogical Value**: Students see the evolution from working code to efficient code\n:::\n\n## Code Efficiency Teaching Resources\n\n::: {.small}\n**Readability and Maintainability (2025 Research):**\n\n- [Code Readability Research](https://www.researchgate.net/publication/390151582_The_Impact_of_Code_Readability_on_Software_Maintenance_efficiency_in_open_source_development) - 58% variance in maintenance efficiency\n- [Cognitive Load Studies](https://dl.acm.org/doi/10.1145/3196321.3196347) - Impact on developer comprehension\n- [Premature Optimization Pitfalls](https://eshman.pro/2025/03/the-pitfalls-of-premature-code-optimization-lessons-from-experience/) - 2025 best practices\n\n**Practical Guidelines:**\n\n- [Stack Overflow: Readability vs Performance](https://stackoverflow.com/questions/183201/should-a-developer-aim-for-readability-or-performance-first) - Community wisdom\n- [Code Quality Principles](https://zencoder.ai/blog/readability-maintainability-in-quality-code) - Professional standards\n- [LinkedIn: Balancing Optimization](https://www.linkedin.com/advice/3/how-do-you-balance-code-optimization-readability) - Industry perspectives\n\n**Teaching Materials:**\n\n- Focus on meaningful variable names and function design\n- Emphasize \"readable first, optimize later\" principle  \n- Use real examples showing readability impact on debugging\n- Teach systematic profiling before optimization\n:::\n\n\n\n# Multi-Language Integration {background-color=\"#0F4C81\"}\n\n## Why R + Python Together?\n\nThe modern data science workflow is increasingly multilingual:\n\n::: {.incremental}\n- **Best tool for the job**\n- **Team collaboration**\n- **Ecosystem access**\n- **Career preparation**\n:::\n\n. . .\n\n::: {style=\"font-size: 1.1em; color: #2E7D32;\"}\n**Teaching philosophy: \"It's not Python vs R, it's Python AND R\"**\n:::\n\n## The `reticulate` + Arrow Stack\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install multilingual tools\n# pak::pak(c(\"reticulate\", \"arrow\", \"palmerpenguins\"))\n\n# Setup Python environment\nlibrary(reticulate)\nlibrary(arrow)\n\n# Modern 2025 approach: automatic Python setup\npy_require(c(\"pandas\", \"numpy\", \"pyarrow\"))\n```\n:::\n\n\n\n\n::: {.midi}\n**2025 Update**: reticulate 1.41 uses `uv` backend for simplified Python environment management\n:::\n\n## Efficient Data Transfer with Arrow\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R: Prepare data as Arrow table\nlibrary(arrow)\npenguins_arrow <- arrow_table(penguins)\n\n# Pass to Python (zero-copy!)\npy$penguins_data <- penguins_arrow\n\n# Python chunk processes data\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# Python: Work with the data\nimport pandas as pd\n\n# Convert from Arrow (still efficient)\ndf = r.penguins_data.to_pandas()\n\n# Python-specific analysis\nfrom sklearn.cluster import KMeans\n# ... machine learning code ...\n\n# Return results to R\nprocessed_data = df.groupby('species').mean()\n```\n:::\n\n\n\n\n## Seamless Language Switching\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R: Statistical modeling\nlibrary(broom)\n\nmodel_results <- penguins %>%\n  filter(!is.na(bill_length_mm), !is.na(body_mass_g)) %>%\n  nest_by(species) %>%\n  mutate(\n    model = list(lm(body_mass_g ~ bill_length_mm, data = data)),\n    tidy_results = list(tidy(model))\n  )\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# Python: Machine learning on same data\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\n# Use the R data directly\nX = r.penguins[['bill_length_mm', 'bill_depth_mm']].dropna()\ny = r.penguins['body_mass_g'].dropna()\n\n# Train model\nrf_model = RandomForestRegressor()\nrf_model.fit(X, y)\n```\n:::\n\n\n\n\n## Teaching Multi-Language Workflows\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R: Data preparation and exploration\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\n# Exploratory analysis in R\npenguins %>%\n  ggplot(aes(bill_length_mm, body_mass_g, color = species)) +\n  geom_point() +\n  labs(title = \"Penguin Measurements by Species\")\n\n# Pass cleaned data to Python for ML\npy$clean_penguins <- penguins %>%\n  drop_na() %>%\n  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, species)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# Python: Machine learning pipeline\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Use R's cleaned data\ndf = r.clean_penguins\nX = df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]\ny = df['species']\n\n# Train classifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\n\n# Return predictions to R\npredictions = clf.predict(X_test)\n```\n:::\n\n\n\n\n## Multi-lingual Packages\n\n::: {style=\"font-size: 1.1em; color: #2E7D32;\"}\n\nSome multi-lingual packages already available:\n\n* `polars` (same for both R and python)\n* `arrow` and `pyarrow`\n* `duckDB` (same for both R and python)\n\n:::\n\nThese have very similar syntax and have the same underlying behavior.\n\n\n## Multi-Language Teaching Resources\n\n::: {.small}\n**Official Documentation & Guides:**\n\n- [Arrow R-Python Integration](https://arrow.apache.org/docs/r/articles/python.html) - Official cross-language guide\n- [reticulate Documentation](https://rstudio.github.io/reticulate/) - Complete R-Python integration\n- [Danielle Navarro's Arrow Tutorial](https://blog.djnavarro.net/posts/2022-09-09_reticulated-arrow/) - Practical examples\n\n**Teaching-Focused Resources:**\n\n- [Atorus Research Multilingual Markdown](https://www.atorusresearch.com/multilingual-markdown-with-r-and-python-using-reticulate/) - Educational approach\n- [Microsoft Data Science Guide](https://medium.com/data-science-at-microsoft/collaborating-between-python-and-r-using-reticulate-25246b367957) - Team collaboration\n- [Flukeandfeather Introduction](https://flukeandfeather.com/posts/2023-03-17-intro-reticulate/) - Beginner-friendly tutorial\n\n**2025 Updates:**\n\n- reticulate 1.41 with uv backend for simplified setup\n- Improved Arrow integration for zero-copy data transfer\n- Growing emphasis on multilingual data science teams\n:::\n\n\n## Professional Development for Instructors\n\n::: {.small}\n**Performance Optimization:**\n\n- [Advanced R Performance](https://adv-r.hadley.nz/perf-improve.html) - Comprehensive optimization guide\n- [R Inferno](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf) - Common performance pitfalls\n- [Efficient R Programming](https://csgillespie.github.io/efficientR/) - Practical optimization strategies\n\n**Code Quality Teaching:**\n\n- [Clean Code Principles](https://www.oreilly.com/library/view/clean-code-a/9780136083238/) - Industry standards\n- [Code Quality Research](https://ieeexplore.ieee.org/document/8811910) - Academic perspectives\n- [Refactoring Techniques](https://refactoring.guru/) - Systematic improvement methods\n\n**Multilingual Data Science:**\n\n- [Python for R Users](https://rstudio-conf-2020.github.io/r-for-excel/) - Transition strategies\n- [Polyglot Data Science](https://www.manning.com/books/effective-data-science-infrastructure) - Team workflows\n- [Cross-Language Best Practices](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510) - Research software guidelines\n:::\n\n## Assessment Ideas for Developer Thinking\n\n::: {.midi}\n**Performance Portfolio:**\n\n- Document before/after optimization with benchmarks\n- Explain trade-offs between performance and readability\n- Profile and optimize a provided slow function\n\n**Code Quality Review:**\n\n- Peer review exercises with readability rubrics\n- Refactor legacy code for better maintainability\n- Write functions with clear interfaces and documentation\n\n**Multilingual Projects:**\n\n- Implement analysis using both R and Python\n- Compare language-specific approaches to same problem\n- Create reproducible multilingual workflow documentation\n\n**Real-World Applications:**\n\n- Optimize code for large datasets\n- Debug performance issues in complex pipelines\n- Collaborate on mixed-language team projects\n:::\n\n",
    "supporting": [
      "08-developer_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}